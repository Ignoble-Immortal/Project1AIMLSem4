{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# !pip install keras\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# !pip install tensorflow\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Download stopwords if not already downloaded\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mnltk\u001b[49m\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstopwords\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# !pip install keras==2.6.0\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# !pip install beautifulsoup4\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# # Call the integrate_summarization function with your CSV file containing URLs\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# integrate_summarization(\"your_csv_file.csv\")\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "# !pip install keras\n",
    "# !pip install tensorflow\n",
    "\n",
    "\n",
    "\n",
    "# !pip install keras==2.6.0\n",
    "# !pip install beautifulsoup4\n",
    "\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd \n",
    "# import re\n",
    "# from bs4 import BeautifulSoup\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "# from attention import AttentionLayer \n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "# from nltk.corpus import stopwords\n",
    "# from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "# from newspaper import Article\n",
    "# import warnings\n",
    "# pd.set_option(\"display.max_colwidth\", 200)\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# # Function to clean text\n",
    "# def text_cleaner(text, num):\n",
    "#     # Your text cleaning code here\n",
    "#     # lower\n",
    "#     newString = text.lower()\n",
    "#     # remove HTML\n",
    "#     newString = BeautifulSoup(newString, \"lxml\").text\n",
    "#     # Remove any text inside the parenthesis\n",
    "#     newString = re.sub(r'\\([^)]*\\)', '', newString)\n",
    "#     # remove double quotes\n",
    "#     newString = re.sub('\"','', newString)\n",
    "#     # contraction mapping\n",
    "#     newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")])  \n",
    "#     # remove 's\n",
    "#     newString = re.sub(r\"'s\\b\",\"\",newString)\n",
    "#     # Eliminate punctuations and special characters\n",
    "#     newString = re.sub(\"[^a-zA-Z]\", \" \", newString)\n",
    "#     # Remove stopwords\n",
    "#     if(num==0):\n",
    "#         tokens = [w for w in newString.split() if not w in stop_words]\n",
    "#     else:\n",
    "#         tokens=newString.split()\n",
    "#     long_words=[]\n",
    "#     # Remove short words\n",
    "#     for i in tokens:\n",
    "#         if len(i)>1:                                                 \n",
    "#             long_words.append(i)   \n",
    "#     return (\" \".join(long_words)).strip()\n",
    "#     pass\n",
    "\n",
    "# # Function to load and preprocess data from URLs\n",
    "# def load_data_from_urls(csv_file):\n",
    "#     data = pd.read_csv(csv_file)\n",
    "#     cleaned_text = []\n",
    "#     for url in data['URL']:\n",
    "#         article = Article(url)\n",
    "#         article.download()\n",
    "#         article.parse()\n",
    "#         text = article.text\n",
    "#         cleaned_text.append(text_cleaner(text, 0))\n",
    "#     data['cleaned_text'] = cleaned_text\n",
    "#     return data\n",
    "\n",
    "# # Function to preprocess text\n",
    "# def preprocess_text(text):\n",
    "#     # Your text preprocessing code here\n",
    "#     pass\n",
    "\n",
    "# # Function to summarize text\n",
    "# def summarize_text(text):\n",
    "#     # Your text summarization code here\n",
    "#     pass\n",
    "\n",
    "# # Function to integrate text summarization with data loading and analysis\n",
    "# def integrate_summarization(csv_file):\n",
    "#     # Load and preprocess data from URLs\n",
    "#     data = load_data_from_urls(csv_file)\n",
    "\n",
    "#     # Preprocess text\n",
    "#     preprocessed_text = data['cleaned_text'].apply(preprocess_text)\n",
    "\n",
    "#     # Tokenization\n",
    "#     # Your tokenization code here\n",
    "\n",
    "#     # Padding sequences\n",
    "#     # Your padding sequences code here\n",
    "\n",
    "#     # Define and train the model\n",
    "#     # Your model definition and training code here\n",
    "\n",
    "#     # Summarize text\n",
    "#     for i in range(len(preprocessed_text)):\n",
    "#         print(\"Original text:\", data['cleaned_text'][i])\n",
    "#         print(\"Summarized text:\", summarize_text(preprocessed_text[i]))\n",
    "#         print('-----------------------------------------------')\n",
    "\n",
    "# # Call the integrate_summarization function with your CSV file containing URLs\n",
    "# integrate_summarization(\"your_csv_file.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ojas Gupta\\AppData\\Local\\Temp\\ipykernel_20432\\512483380.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 200\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# Calculate maximum summary length\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m max_summary_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(summary\u001b[38;5;241m.\u001b[39msplit()) \u001b[38;5;28;01mfor\u001b[39;00m summary \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdata\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_summary\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    202\u001b[0m \u001b[38;5;66;03m# Load and preprocess data from URLs\u001b[39;00m\n\u001b[0;32m    203\u001b[0m data \u001b[38;5;241m=\u001b[39m load_data_from_urls(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCNNLinks.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# from attention import AttentionLayer\n",
    "from tensorflow.keras.layers import Attention\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from newspaper import Article\n",
    "import warnings\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "contraction_mapping = {\n",
    "    \"ain't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"can't've\": \"cannot have\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he'll've\": \"he will have\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"I'd\": \"I would\",\n",
    "    \"I'd've\": \"I would have\",\n",
    "    \"I'll\": \"I will\",\n",
    "    \"I'll've\": \"I will have\",\n",
    "    \"I'm\": \"I am\",\n",
    "    \"I've\": \"I have\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"i'd've\": \"i would have\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i'll've\": \"i will have\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it'll've\": \"it will have\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she'll've\": \"she will have\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so is\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there would\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they'll've\": \"they will have\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what'll've\": \"what will have\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who'll've\": \"who will have\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you'll've\": \"you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "# Function to clean text\n",
    "def text_cleaner(text, num):\n",
    "    # Lowercase\n",
    "    newString = text.lower()\n",
    "    # Remove HTML\n",
    "    newString = BeautifulSoup(newString, \"lxml\").text\n",
    "    # Remove any text inside the parenthesis\n",
    "    newString = re.sub(r'\\([^)]*\\)', '', newString)\n",
    "    # Remove double quotes\n",
    "    newString = re.sub('\"','', newString)\n",
    "    # Contraction mapping\n",
    "    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")])  \n",
    "    # Remove 's\n",
    "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
    "    # Eliminate punctuations and special characters\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString)\n",
    "    # Remove stopwords\n",
    "    if(num==0):\n",
    "        tokens = [w for w in newString.split() if not w in stop_words]\n",
    "    else:\n",
    "        tokens=newString.split()\n",
    "    long_words=[]\n",
    "    # Remove short words\n",
    "    for i in tokens:\n",
    "        if len(i)>1:                                                 \n",
    "            long_words.append(i)   \n",
    "    return (\" \".join(long_words)).strip()\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# Define stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to load and preprocess data from URLs\n",
    "def load_data_from_urls(csv_file):\n",
    "    data = pd.read_csv(csv_file)\n",
    "    cleaned_text = []\n",
    "    cleaned_summary = []\n",
    "    for url in data['URL']:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        text = article.text\n",
    "        summary = article.summary\n",
    "        cleaned_text.append(text_cleaner(text, 0))\n",
    "        cleaned_summary.append(text_cleaner(summary, 1))\n",
    "    data['cleaned_text'] = cleaned_text\n",
    "    data['cleaned_summary'] = cleaned_summary\n",
    "    return data\n",
    "\n",
    "# Calculate maximum summary length\n",
    "max_summary_len = max(len(summary.split()) for summary in data['cleaned_summary'])\n",
    "\n",
    "# Load and preprocess data from URLs\n",
    "data = load_data_from_urls(\"CNNLinks.csv\")\n",
    "\n",
    "# Split data into train and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_tr, x_val, y_tr, y_val = train_test_split(data['cleaned_text'], data['cleaned_summary'], test_size=0.1, random_state=0, shuffle=True)\n",
    "\n",
    "# Tokenization\n",
    "x_tokenizer = Tokenizer() \n",
    "x_tokenizer.fit_on_texts(list(x_tr))\n",
    "x_tr_seq = x_tokenizer.texts_to_sequences(x_tr) \n",
    "x_val_seq = x_tokenizer.texts_to_sequences(x_val)\n",
    "# Calculate max_text_len\n",
    "max_text_len = max(len(sequence) for sequence in x_tr_seq + x_val_seq)\n",
    "\n",
    "x_tr = pad_sequences(x_tr_seq, maxlen=max_text_len, padding='post')\n",
    "x_val = pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')\n",
    "# Set num_words attribute\n",
    "x_tokenizer.num_words = 5000\n",
    "x_voc = x_tokenizer.num_words + 1\n",
    "\n",
    "y_tokenizer = Tokenizer()   \n",
    "y_tokenizer.fit_on_texts(list(y_tr))\n",
    "y_tr_seq = y_tokenizer.texts_to_sequences(y_tr) \n",
    "y_val_seq = y_tokenizer.texts_to_sequences(y_val) \n",
    "y_tr = pad_sequences(y_tr_seq, maxlen=max_summary_len, padding='post')\n",
    "y_val = pad_sequences(y_val_seq, maxlen=max_summary_len, padding='post')\n",
    "y_voc = y_tokenizer.num_words + 1\n",
    "\n",
    "# Define the model architecture\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_text_len,))\n",
    "enc_emb =  Embedding(x_voc, embedding_dim, trainable=True)(encoder_inputs)\n",
    "encoder_lstm1 = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "encoder_lstm2 = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "encoder_lstm3 = LSTM(latent_dim, return_state=True, return_sequences=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm3(encoder_output2)\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(y_voc, embedding_dim, trainable=True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.2)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
    "\n",
    "# attn_layer = AttentionLayer()\n",
    "# attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
    "attn_out = Attention()([encoder_outputs, decoder_outputs])\n",
    "\n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "decoder_dense = TimeDistributed(Dense(y_voc, activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_concat_input)\n",
    "\n",
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# Train the model\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\n",
    "history = model.fit(\n",
    "    [x_tr, y_tr[:,:-1]], \n",
    "    y_tr.reshape(y_tr.shape[0], y_tr.shape[1], 1)[:,1:],\n",
    "    epochs=50,\n",
    "    callbacks=[es],\n",
    "    batch_size=128, \n",
    "    validation_data=([x_val,y_val[:,:-1]], y_val.reshape(y_val.shape[0],y_val.shape[1], 1)[:,1:])\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "from matplotlib import pyplot as plt\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Reverse mapping for decoding\n",
    "reverse_target_word_index = y_tokenizer.index_word\n",
    "reverse_source_word_index = x_tokenizer.index_word\n",
    "target_word_index = y_tokenizer.word_index\n",
    "\n",
    "# Encode input sequence and get initial decoder state\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# Decoder setup\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_hidden_state_input = Input(shape=(max_text_len, latent_dim))\n",
    "\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "decoder_outputs2 = decoder_dense(decoder_inf_concat)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input, decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2]\n",
    ")\n",
    "\n",
    "# Function to decode sequence\n",
    "def decode_sequence(input_seq):\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = target_word_index['sostok']\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
    "        if(sampled_token!='eostok'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "        if (sampled_token == 'eostok'  or len(decoded_sentence.split()) >= (max_summary_len-1)):\n",
    "            stop_condition = True\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "        e_h, e_c = h, c\n",
    "    return decoded_sentence\n",
    "\n",
    "# Function to convert sequence to summary\n",
    "def seq2summary(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=target_word_index['sostok']) and i!=target_word_index['eostok']):\n",
    "            newString=newString+reverse_target_word_index[i]+' '\n",
    "    return newString\n",
    "\n",
    "# Function to convert sequence to text\n",
    "def seq2text(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            newString=newString+reverse_source_word_index[i]+' '\n",
    "    return newString\n",
    "\n",
    "# Generate and print summaries\n",
    "for i in range(10):\n",
    "    print(\"Review:\", seq2text(x_tr[i]))\n",
    "    print(\"Original summary:\", seq2summary(y_tr[i]))\n",
    "    print(\"Predicted summary:\", decode_sequence(x_tr[i].reshape(1, max_text_len)))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
