{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install os\n",
    "from os import listdir\n",
    "import string\n",
    "from pickle import dump,load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadData:\n",
    "    def __init__(self, directory):\n",
    "        self.directory= directory\n",
    "        \n",
    "    def load_stories(self):\n",
    "        \"\"\"\n",
    "        Load the data and store it in a list of dictionaries\n",
    "        \n",
    "        \"\"\"\n",
    "        all_stories= list()\n",
    "        \n",
    "        def load_doc(filename):\n",
    "            \"\"\"\n",
    "            Return the data from a given filename\n",
    "            \"\"\"\n",
    "            file = open(filename, encoding='utf-8')\n",
    "            text = file.read()\n",
    "            file.close()\n",
    "            return text\n",
    "        \n",
    "        def split_story(doc):\n",
    "            \"\"\"\n",
    "            Split story from summaries based on the separater -> \"@highlight\"\n",
    "            \"\"\"\n",
    "            index = doc.find('@highlight')\n",
    "            story, highlights = doc[:index], doc[index:].split('@highlight')\n",
    "            highlights = [h.strip() for h in highlights if len(h) > 0]\n",
    "            return story, highlights\n",
    "        \n",
    "        list_of_files= listdir(self.directory)\n",
    "        for name in list_of_files[:1000]:\n",
    "            filename = self.directory + '/' + name\n",
    "            doc = load_doc(filename)\n",
    "            story, highlights= split_story(doc)\n",
    "            all_stories.append({'story': story, 'highlights': highlights})\n",
    "        \n",
    "        return all_stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_PATH= \"/home/nikhil/Downloads/cnn/stories\"\n",
    "obj= LoadData(DIR_PATH)\n",
    "stories= obj.load_stories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stories[10]['highlights'])\n",
    "print()\n",
    "print(stories[10]['story'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clean_data:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "           \n",
    "    def clean_lines(self, lines):\n",
    "        cleaned = list()\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        \n",
    "        for line in lines:\n",
    "            index = line.find('(CNN)')\n",
    "            if index >= 0:\n",
    "                line = line[index + len('(CNN)'):]\n",
    "\n",
    "            split_line = line.split()\n",
    "            \n",
    "            split_line = [word.lower() for word in split_line]\n",
    "            split_line = [w.translate(table) for w in split_line]\n",
    "            \n",
    "            split_line = [word for word in split_line if word.isalpha()]\n",
    "            cleaned.append(' '.join(split_line))\n",
    "        cleaned = [c for c in cleaned if len(c) > 0]\n",
    "        return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj1= Clean_data()\n",
    "cleaned_stories= list()\n",
    "for example in stories[:100]:\n",
    "    cleaned_stories.append({'story': obj1.clean_lines(example['story'].split('\\n')), 'highlights': obj1.clean_lines(example['highlights'])})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_stories[60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(cleaned_stories, open('/home/nikhil/Downloads/cnn/processed_sample_data/cnn_dataset.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_stories = load(open('/home/nikhil/Downloads/cnn/processed_sample_data/cnn_dataset.pkl', 'rb'))\n",
    "print('Loaded Stories %d' % len(cleaned_stories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Food reviews Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMAZON_DATA_PATH= '/home/nikhil/Downloads/amazon-fine-food-reviews/Reviews.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Load_amazon_data:\n",
    "    def __init__(self, dir_path, seed= 0):\n",
    "        self.dir_path= dir_path\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "    def load(self):\n",
    "        \"\"\"\n",
    "        Reads data from the given directory path\n",
    "        \"\"\"\n",
    "        return pd.read_csv(self.dir_path)\n",
    "    \n",
    "    def drop(self):\n",
    "        \"\"\"\n",
    "        Drops unnecessary columns\n",
    "        \"\"\"\n",
    "        \n",
    "        data= self.load()\n",
    "        \n",
    "        data = data.dropna()\n",
    "        data= data.iloc[:, -2:]\n",
    "        data = data.reset_index(drop= True)\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def analyze_data(self):\n",
    "        \"\"\"\n",
    "        Prints some sample data points from the cleaned data\n",
    "        \"\"\"\n",
    "        data= self.drop()\n",
    "        \n",
    "        for sr_no, i in enumerate(np.random.randint(10, 100, size= 5)):\n",
    "            print(\"_________________________\")\n",
    "            print(\"Data Point {0}\".format(sr_no + 1))\n",
    "            print(\"Summary:\")\n",
    "            print(data['Summary'].iloc[i])\n",
    "            print(\"Full Text:\")\n",
    "            print(data['Text'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj= Load_amazon_data(AMAZON_DATA_PATH, seed= 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= obj.load()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping Unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= obj.drop()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj.analyze_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_cleaning:\n",
    "    def __init__(self):\n",
    "        self.clean_summaries= []\n",
    "        self.clean_texts= []\n",
    "\n",
    "    def clean_text(self, text, remove_stopwords = False):\n",
    "        \"\"\"\n",
    "        Defines a series of cleaning operations \n",
    "        \"\"\"\n",
    "        text = text.lower()\n",
    "\n",
    "        if True:\n",
    "            text = text.split()\n",
    "            new_text = []\n",
    "            for word in text:\n",
    "                if word in contractions:\n",
    "                    new_text.append(contractions[word])\n",
    "                else:\n",
    "                    new_text.append(word)\n",
    "            text = \" \".join(new_text)\n",
    "\n",
    "        text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "        text = re.sub(r'\\<a href', ' ', text)\n",
    "        text = re.sub(r'&amp;', '', text) \n",
    "        text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "        text = re.sub(r'<br />', ' ', text)\n",
    "        text = re.sub(r'<br >', ' ', text)\n",
    "        text = re.sub(r'<br  >', ' ', text)\n",
    "        text = re.sub(r'\\'', ' ', text)\n",
    "\n",
    "        # Optionally, remove stop words\n",
    "        if remove_stopwords:\n",
    "            text = text.split()\n",
    "            stops = set(stopwords.words(\"english\"))\n",
    "            text = [w for w in text if not w in stops]\n",
    "            text = \" \".join(text)\n",
    "\n",
    "        return text\n",
    "    \n",
    "    def clean(self, data):\n",
    "        \"\"\"\n",
    "        Applies the clean_text() to the entire dataset\n",
    "        \"\"\"\n",
    "        for summary in data.Summary:\n",
    "            self.clean_summaries.append(self.clean_text(summary))\n",
    "\n",
    "        print(\"Summaries are complete.\")\n",
    "\n",
    "        for text in data.Text:\n",
    "            self.clean_texts.append(self.clean_text(text))\n",
    "\n",
    "        print(\"Texts are complete.\")\n",
    "        \n",
    "        return self.clean_summaries, self.clean_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "clean_obj= Data_cleaning()\n",
    "clean_summaries, clean_texts= clean_obj.clean(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "for sr_no, i in enumerate(np.random.randint(10, 100, size= 5)):\n",
    "    print(\"_________________________\")\n",
    "    print(\"Data Point #{0}\".format(sr_no + 1))\n",
    "    print(\"Summary:\")\n",
    "    print(clean_summaries[i])\n",
    "    print(\"Full Text:\")\n",
    "    print(clean_texts[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
